# autograd-zero
A tiny, easy-to-understand automatic differentiation engine for scalar values..
---

##  Whatâ€™s Working Right Now

This project is a minimal clone of pytorch autograd engine but with scaler values.

### âœ… Currently Implemented:

- ğŸ§© **Core Neuron class**  
  Tracks value, gradient, and the operation used to create each node.

- â• **Basic math operations**  
  You can do `+`, `-`, `*`, and even `**` (power) between Neurons or numbers.

- ğŸ” **Backpropagation (Autodiff)**  
  Computes gradients using reverse-mode autodiff with a topological sort.

- ğŸ“Š **Graph Visualization**  
  Generate computation graphs using **Graphviz** for a better understanding of dependencies.

---

## ğŸ› ï¸ Whatâ€™s Coming Next (v2 Ideas)

- ğŸ”¢ More operations: `/`, `//`, `%`, etc.
  (Done on 15 July 2025) 
- ğŸ”¥ Activation functions: `relu`, `tanh`, `sigmoid`, `lrelu`,(all of these work for forward pass but not for backward pass) and more
  (Tanh,Relu,Sigmoid implementation doen on 15 July 2025) 
- ğŸ¯ Loss functions like MSE,
- ğŸ§¹ `zero_grad()` utility  
- ğŸ§® Vector and matrix support for batched computation

---

## ğŸ“š Why am I doing this?

To truly understand backpropagation and how automatic differentiation and DAG works under the hood.
Learning Topological sort is cherry on top.

---

## ğŸ“„ License

MIT â€” Free to use, share, and modify. Built for learning.
